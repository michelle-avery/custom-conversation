# Supported providers
# ===========================
Support for multiple providers is provided by LiteLLM, but as each provider has its own quirks, this project has only implemented a subset of the available providers. If LiteLLM [supports a provider](https://docs.litellm.ai/docs/providers) you would like to see added, please open an issue.
## Natively supported providers
### OpenAI
OpenAI is supported natively. The base URL is configurable to support proxies or other providers that are compatible with the OpenAI API. Note that supposedly "OpenAI-compatible" APIs are sometimes not fully compatible. Assuming the API is fully compatible, streaming and tool calling are supported.
### Gemini via Google AI Studio
Gemini is supported via the Google AI Studio API. The base URL is not currently configurable due to a LiteLLM [bug](https://github.com/BerriAI/litellm/issues/7830). This will be enabled once the underlying bug is fixed. Streaming and tool calling are supported.

## Support through the OpenAI API
### Ollama
Native Ollama support has been partially implemented, but is currently disabled pending a few LiteLLM bug fixes. The Ollama API is 
compatible with the OpenAI API, so it is possible to use the OpenAI-supported features of Ollama through that API. Streaming and tool 
calling are supported, and some additional tool calling parameter processing is in place to correct the incorrect parameters sometimes 
generated by small LLM models. Keep in mind, tool calling with these models is still not as reliable as with larger models.

### Gemini via Google's OpenAI API
There are some workarounds in place for known compatibility issues with Google's OpenAI implementation, but due to this, using the 
native integration is recommended.